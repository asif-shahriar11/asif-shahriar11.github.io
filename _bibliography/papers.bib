---
---

@string{aps = {American Physical Society,}}

@Article{shahriar20255GPT,
  selected={true},
  bibtex_show={true},
  abbr={IEEE TIFS},
  title="{5GPT: 5G Vulnerability Detection by Combining Zero-Shot Capabilities of GPT-4 With Domain Aware Strategies Through Prompt Engineering}",
  author={Shahriar, Asif and Hisham, Syed Jarullah and Rahman, K. M. Asifur and Islam, Ruhan and Hossain, Md. Shohrab and Hwang, Ren-Hung and Lin, Ying-Dar},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={20},
  pages={7045--7060},
  year={2025},
  doi={10.1109/TIFS.2025.3586480},
  pdf={5GPT_final.pdf},
  slides={5GPT_presentation.pdf},
  preview={domain-aware-image.png}
}




@inproceedings{shahriar2025inceptivetransformers,
  bibtex_show={true},
  abbr={EMNLP 2025},
  title={Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages},
  author={Shahriar, Asif and Shahriyar, Rifat and Rahman, M Saifur},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  year={2025},
  preview={inceptive_transformer_flowchart.png}
}


@inproceedings{shahriar2024xlnetcnn,
bibtex_show={true},
abbr={NSysS 2024},
author = {Shahriar, Asif and Pandit, Debojit and Rahman, M Saifur},
title = {XLNet-CNN: Combining Global Context Understanding of XLNet with Local Context Capture through Convolution for Improved Multi-Label Text Classification},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704522.3704540},
doi = {10.1145/3704522.3704540},
abstract = {Multi-label text classification (MLTC) is the task of assigning multiple relevant labels to a text, which is particularly challenging due to the complex interdependencies between labels and the imbalanced distribution of label frequencies. Domain-specific BERT variants, such as BioBERT, CT-BERT, and HateBERT, are pretrained on specialized corpora, which enables them to capture the unique terminology and patterns within specific domains, thus enhancing their performance in MLTC. However, pretraining BERT variants on specialized corpora is computationally expensive and limits their generalizability to broader tasks, necessitating models that can leverage both domain-specific insights and general-purpose context efficiently. With this in view, we propose XLNet-CNN, which is based on XLNet, an autoregressive transformer model designed to capture long-range dependencies through permutation-based training. Our model enhances XLNet’s global context understanding by integrating a 1D CNN layer to better capture local dependencies and patterns within the text. This combination allows the model to recognize important phrases and word combinations, which are crucial for multi-label text classification. Our experiments on three distinct datasets — Ohsumed (medical abstracts), CAVES (anti-COVID vaccine tweets), and HateXplain (cyberbullying detection)—demonstrate that XLNet-CNN consistently outperforms XLNet and domain-specific BERT models in terms of F1-score. The detailed code and preprocessed data can be found in our github repository: https://github.com/asif-shahriar11/XLNet-CNN.},
booktitle = {Proceedings of the 11th International Conference on Networking, Systems, and Security},
pages = {24–31},
numpages = {8},
pdf={XLNet-CNN-NSysS.pdf},
award_name={Best paper award}
}



