---
---

@string{aps = {American Physical Society,}}

@Article{shahriar20255GPT,
  bibtex_show={true},
  abbr={IEEE TIFS},
  title="{5GPT: 5G Vulnerability Detection by Combining Zero-Shot Capabilities of GPT-4 With Domain Aware Strategies Through Prompt Engineering}",
  author={Shahriar, Asif and Hisham, Syed Jarullah and Rahman, K. M. Asifur and Islam, Ruhan and Hossain, Md. Shohrab and Hwang, Ren-Hung and Lin, Ying-Dar},
  abstract={Identifying vulnerabilities in complex 5G network protocols is a challenging task. Manual analysis is time-consuming and often inadequate. Modern ML and NLP methods, though effective, are resource-intensive and struggle to find implicit vulnerabilities. In this research, we utilize GPT-4’s advanced language understanding to detect vulnerabilities directly from 5G specifications. To assess GPT-4’s fundamental capabilities in this domain, we first adopt a zero-shot approach that relies solely on the specification text without external guidance. For detecting more sophisticated vulnerabilities that require deep contextual understanding, we introduce a novel domain-aware strategy, where we explicitly teach GPT-4 about security properties and hazard indicators from related works using few-shot learning. We further employ chain-of-thought prompting to guide the model through structured reasoning steps to identify violations or exploitations that may lead to vulnerabilities. A two-tier filtering process ensures that only promising test-cases are retained. Our method has identified 47 potential vulnerabilities in 5G mobility management procedures, including 27 previously unreported issues, and generated corresponding test-cases. Simulating 14 of them, we have found 9 vulnerabilities, five of which are new. The zero-shot approach is effective in detecting procedural and validation flaws, while the domain-aware method excels in finding protocol violations and advanced attack scenarios. These findings validate our methodology and demonstrate its strength in discovering both known and novel vulnerabilities in 5G protocols.},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={20},
  number={},
  pages={7045--7060},
  year={2025},
  doi={10.1109/TIFS.2025.3586480}
}




@inproceedings{shahriar2025inceptivetransformers,
  bibtex_show={true},
  abbr={EMNLP 2025},
  title={Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages},
  author={Shahriar, Asif and Shahriyar, Rifat and Rahman, M Saifur},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  abstract={Conventional transformer models typically compress the information from all tokens in a sequence into a single [CLS] token to represent global context— an approach that can lead to information loss in tasks requiring localized or hierarchical cues. In this work, we introduce Inceptive Transformers, a modular and lightweight architecture that enriches transformer-based token representations by integrating a multi-scale feature extraction module inspired by inception networks. Our model is designed to balance local and global dependencies by dynamically weighting tokens based on their relevance to a particular task. Evaluation across a diverse range of tasks including emotion recognition (both English and Bangla), irony detection, disease identification, and anti-COVID vaccine tweets classification shows that our models consistently outperform the baselines by 1% to 14% while maintaining efficiency. These findings highlight the versatility and cross-lingual applicability of our method for enriching transformer-based representations across diverse domains.}
}



